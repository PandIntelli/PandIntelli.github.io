

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/aquaicon.png">
  <link rel="icon" href="/img/aquaicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Pand">
  <meta name="keywords" content="Hexo, Contrastive Learning, Research">
  
    <meta name="description" content="框架对于神经网络训练来讲必不可少。最近对 PyTorch 进行了粗略的学习，根据官方文档和网络博客对学到的内容进行了简单梳理，方便之后查阅。">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch Tutorial">
<meta property="og:url" content="https://pandintelli.github.io/2022/01/13/Pytorch-Tutorial/index.html">
<meta property="og:site_name" content="Pand&#96;s Blog">
<meta property="og:description" content="框架对于神经网络训练来讲必不可少。最近对 PyTorch 进行了粗略的学习，根据官方文档和网络博客对学到的内容进行了简单梳理，方便之后查阅。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pandintelli.github.io/index_img/pytorch_tutorial.jpg">
<meta property="article:published_time" content="2022-01-13T08:23:15.000Z">
<meta property="article:modified_time" content="2023-04-21T11:53:18.450Z">
<meta property="article:author" content="Pand">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://pandintelli.github.io/index_img/pytorch_tutorial.jpg">
  
  
  
  <title>Pytorch Tutorial - Pand`s Blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"pandintelli.github.io","root":"/","version":"1.9.4","typing":{"enable":false,"typeSpeed":50,"cursorChar":"","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Pand</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">Pytorch Tutorial</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-01-13 16:23" pubdate>
          2022年1月13日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          28k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          <!-- compatible with older versions-->
          232 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Pytorch Tutorial</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="基础元素-Tensor"><a href="#基础元素-Tensor" class="headerlink" title="基础元素 Tensor"></a>基础元素 Tensor</h2><p>Tensor 是 PyTorch 中的一个基础数据结构，和 numpy 中的 ndarray 非常相近，我们使用 tensors 去编码输入和输出，还有模型参数。</p>
<h3 id="生成操作"><a href="#生成操作" class="headerlink" title="生成操作"></a>生成操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">x_ones = torch.ones_like(data) <span class="hljs-comment"># 生成和 data 大小一样的全1 tensor</span><br>x_rand = torch.rand_like(data) <span class="hljs-comment"># 生成和 data 大小一样的随机 tensor (value 0~1)</span><br><br>shape = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>] <span class="hljs-comment"># 指定大小</span><br>rand_tensor = torch.rand(shape)<br>zeros_tensor = torch.zeros(shape)<br>ones_tensor = torch.ones(shape)<br></code></pre></td></tr></table></figure>

<h3 id="Tensor-属性"><a href="#Tensor-属性" class="headerlink" title="Tensor 属性"></a>Tensor 属性</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = torch.rand(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)<br>tensor.shape <span class="hljs-comment"># Shape of tensor</span><br>tensor.dtype <span class="hljs-comment"># Datatype of tensor</span><br>tensor.device <span class="hljs-comment"># Device that tensor is stored on</span><br></code></pre></td></tr></table></figure>

<h3 id="Tensor-操作"><a href="#Tensor-操作" class="headerlink" title="Tensor 操作"></a>Tensor 操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 索引, 切片操作和 Numpy 一致</span><br><span class="hljs-comment"># join 操作</span><br>t1 = torch.cat([tensor, tensor, tensor], dim=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 算术操作</span><br><br><span class="hljs-comment"># tensors 之间的矩阵乘法, y1, y2, y3 会有相同的值 </span><br>y1 = tensor @ tensor.T<br>y2 = tensor.matmul(tensor.T)<br>torch.matmul(tensor, tensor.T, out=y3)<br><br><span class="hljs-comment"># tensors 之间的 element-wise product (点乘), z1, z2, z3 会有相同的只</span><br>z1 = tensor * tensor<br>z2 = tensor.mul(tensor)<br>torch.mul(tensor, tensor, out=z3)<br><br><span class="hljs-comment"># 单个元素取值操作, 当只有一个元素时, 可以将它转换成Python数值</span><br>value = value.item()<br><br><span class="hljs-comment"># In-place 操作, 会替换之前元素的存储空间, 在其他操作后加上 _ 后缀</span><br>tensor.add_(<span class="hljs-number">5</span>)<br>tensor.copy_(x)<br>tensor.t_(x)<br></code></pre></td></tr></table></figure>

<h3 id="和-Numpy-的关联"><a href="#和-Numpy-的关联" class="headerlink" title="和 Numpy 的关联"></a>和 Numpy 的关联</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Tensor to Numpy</span><br>np = tensor.numpy()<br><span class="hljs-comment"># Numpy to Tensor</span><br>tensor = torch.from_numpy(np)<br></code></pre></td></tr></table></figure>





<h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>PyTorch 处理数据有两个前提：<code>torch.utils.data.Dataset</code> 和 <code>torch.utils.data.DataLoader</code>。<code>Dataset</code> 存储样本和它们对应的标签，<code>DataLoader</code> 对 <code>Dataset</code> 绑定迭代轮数。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><code>torchvision.datasets</code> 模块包含许多真实世界图像数据如 CIFAR, COCO (<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/datasets.html">full list here</a>)。</p>
<p>所有数据集都是 <code>torch.utils.data.Dataset</code> 的子类，也就是说，它们都有 <code>__getitem__</code> 和 <code>__len__</code> 这两个API。所以，它们可以通过 <code>torch.multiprocessing</code> 给 <code>torch.utils.data.DataLoader</code> 传递多个样本实现多线程。</p>
<p>所有数据集都有类似的API。它们有两个共同参数 <code>transform</code> 和 <code>target_transform</code> 来分别转换 input 和 target。可以通过提供的 <a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/datasets.html#base-classes-datasets">base classes</a> 创建自己的数据集。</p>
<h4 id="导入官方提供数据集"><a href="#导入官方提供数据集" class="headerlink" title="导入官方提供数据集"></a>导入官方提供数据集</h4><p>此处以 <code>ImageNet ILSVRC2012</code> 数据集举例，但 PyTorch 不提供 ImageNet 数据集的下载，需要自己下载数据集后放在root路径，此处放在 <code>./dataset</code> 路径下。如果 PyTorch 有提供数据集下载，<code>download=True</code> 后会将数据集下载在 <code>root</code> 路径下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor<br><br><span class="hljs-comment"># training_data = datasets.ImageNet(root=&quot;dataset&quot;, train=True, download=True, transform=ToTensor())</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">download = True 会报错, 原因上述已经提到, PyTorch 库中代码根本没有下载, 源代码如下</span><br><span class="hljs-string">if download is True:</span><br><span class="hljs-string">	msg = (&quot;The dataset is no longer publicly accessible. You need to &quot;</span><br><span class="hljs-string">		   &quot;download the archives externally and place them in the root &quot;</span><br><span class="hljs-string">           &quot;directory.&quot;)</span><br><span class="hljs-string">	raise RuntimeError(msg)</span><br><span class="hljs-string">elif download is False:</span><br><span class="hljs-string">    msg = (&quot;The use of the download flag is deprecated, since the dataset &quot;</span><br><span class="hljs-string">           &quot;is no longer publicly accessible.&quot;)</span><br><span class="hljs-string">    warnings.warn(msg, RuntimeWarning)</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br>training_data = datasets.ImageNet(root=<span class="hljs-string">&quot;dataset&quot;</span>, train=<span class="hljs-literal">True</span>, transform=ToTensor())<br>test_data = datasets.ImageNet(root=<span class="hljs-string">&quot;dataset&quot;</span>, train=<span class="hljs-literal">False</span>, transform=ToTensor())<br></code></pre></td></tr></table></figure>

<h4 id="创建自定义数据集"><a href="#创建自定义数据集" class="headerlink" title="创建自定义数据集"></a>创建自定义数据集</h4><p><code>Dataset</code> 类的部分内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Dataset</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;An abstract class representing a Dataset.</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    All other datasets should subclass it. All subclasses should overide ``__len__``,     that provides the size of the dataset, and ``__getitem__``, supporing integer     	indexing in range from 0 to len(self) exclusive.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br></code></pre></td></tr></table></figure>

<p>子类继承父类 <code>Dataset</code> 时，必须重写 <code>__getitem__</code> 和 <code>__len__</code> 方法，否则将会报错。</p>
<p>其中 <code>__getitem__</code> 实现通过索引来返回图像数据的功能，<code>__len__</code> 返回数据集的大小。</p>
<p>要自定义数据集，首先要继承 <code>Dataset</code> 类，然后在 <code>__init__</code> 方法中对数据进行整理，给图片打标签，划分数据集等等。</p>
<p>Example：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PCIEDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, root, size, mode</span>):<br>        <span class="hljs-built_in">super</span>(PCIEDataset, self).__init__()<br>        self.root = root<br>        self.size = size<br>        <br>        self.images, self.labels = load_images(self.root) <span class="hljs-comment"># 从路径中读取文件</span><br>        <span class="hljs-comment"># 图像处理, 任意什么处理都可以, 这里只是简单 resize</span><br>        self.images = [x.reshape(self.size) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> self.images] <span class="hljs-comment"># 将图片resize</span><br>        <br>        <span class="hljs-comment"># 对数据集进行划分</span><br>        <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-comment"># 60%</span><br>            self.images = self.images[:<span class="hljs-built_in">int</span>(<span class="hljs-number">0.6</span>*<span class="hljs-built_in">len</span>(self.images))]<br>            self.labels = self.labels[:<span class="hljs-built_in">int</span>(<span class="hljs-number">0.6</span>*<span class="hljs-built_in">len</span>(self.labels))]<br>        <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;val&#x27;</span>: <span class="hljs-comment"># 40%</span><br>            self.images = self.images[-<span class="hljs-built_in">int</span>(<span class="hljs-number">0.4</span>*<span class="hljs-built_in">len</span>(self.images))]<br>            self.labels = self.labels[-<span class="hljs-built_in">int</span>(<span class="hljs-number">0.4</span>*<span class="hljs-built_in">len</span>(self.labels))]<br>            <br>	<span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<br>        img, label = self.images[index], self.labels[index]<br>        <span class="hljs-keyword">return</span> torch.tensor(img), torch.tensor(label)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.images)<br></code></pre></td></tr></table></figure>

<h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h3><p><code>Dataset</code> 每次检索提供一个样本和一个标签，在训练模型时，我们通常通过 minibatches 来取数据集，在每个 epoch 重新打乱数据来避免模型过拟合，并且使用 Python 的 <code>multiprocessing</code> 对数据检索加速。</p>
<p>DataLoader 类的定义如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">torch</span>.utils.data.DataLoader(<br>	dataset,								<span class="hljs-comment"># 加载数据的数据集</span><br>    batch_size=<span class="hljs-number">1</span>,							<span class="hljs-comment"># 每个 batch 加载多少个样本</span><br>    shuffle=<span class="hljs-literal">False</span>,							<span class="hljs-comment"># 在每个 epoch 重新打乱数据</span><br>    sampler=<span class="hljs-literal">None</span>,							<span class="hljs-comment"># 从数据集中生成 index 的方式</span><br>    batch_sampler=<span class="hljs-literal">None</span>,						<br>    num_workers=<span class="hljs-number">0</span>,							<span class="hljs-comment"># 用多少个子进程加载数据, 0 默认单进程</span><br>    collate_fn=&lt;function default_collate&gt;,	<span class="hljs-comment"># 将一个 batch 的数据集和标签进行合并操作</span><br>    pin_memory=<span class="hljs-literal">False</span>,						<span class="hljs-comment"># True 时, 最开始生成的内存属于锁页内存, 转义到GPU的显存速度会更快一点</span><br>    drop_last=<span class="hljs-literal">False</span>							<br>    <span class="hljs-comment"># 如果数据集大小不能被 batchsize 整除, 则为True后可以删除最后一个不完整的 batch, 如果为False, 则最后一个 batch 将会更小 </span><br>)<br></code></pre></td></tr></table></figure>



<p>Example：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">train_loader = DataLoader(dataset, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, pin_memory=<span class="hljs-literal">True</span>)<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span>, label <span class="hljs-keyword">in</span> train_loader:<br>        <span class="hljs-comment"># 训练操作</span><br></code></pre></td></tr></table></figure>



<h2 id="变换-Transforms"><a href="#变换-Transforms" class="headerlink" title="变换 (Transforms)"></a>变换 (Transforms)</h2><p>由于大多数据在进行训练之前都需要进行处理，PyTorch 内置了一些函数，来方便用户对图像等数据进行处理。</p>
<p>以下提到的类都在 <code>torchvision.transforms</code> 下，例如 <code>torchvision.transforms.Compose</code></p>
<h3 id="裁剪"><a href="#裁剪" class="headerlink" title="裁剪"></a>裁剪</h3><h4 id="CenterCrop"><a href="#CenterCrop" class="headerlink" title="CenterCrop"></a>CenterCrop</h4><p>将给定的 <code>PIL.Image</code> 进行中心切割，得到给定的 size。 size 可以是一个 tuple ([height, width])，也可以是一个整数，在整数的情况下，切割出来的图片形状是正方形。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">CenterCrop([h, w])<br>CenterCrop(size)<br></code></pre></td></tr></table></figure>

<h4 id="RandomCrop"><a href="#RandomCrop" class="headerlink" title="RandomCrop"></a>RandomCrop</h4><p>切割中心点的位置随便选取，根据给定的 size 进行切割，具体参数如下。</p>
<ul>
<li>size：可以是 tuple 也可以是整数。</li>
<li>padding：设置填充多少个 pixel。当为 <code>int</code> 时，图像上下左右均填充 <code>int</code> 个，若有两个数，则第一个数为左右扩充多少，第二个数表示上下的。当有 4 个数时，则为 左、上、右、下 各填充多少个。</li>
<li>fill：填充的值 (仅当填充 mode 为 ‘constant’ 时有效。当为 <code>int</code> 时，各通道均填充该值，当为 [r, g, b] tuple 时，表示RGB通道分别填充的值。</li>
<li>padding_mode：有 4 种填充模式：<ul>
<li>constant 常量。</li>
<li>edge 按照图片边缘像素值填充。</li>
<li>reflect 。以边缘值为中心做镜面对称，如 [1, 2, 3, 4] 边缘扩充一个值，结果为 [2, 1, 2, 3, 4, 3]。</li>
<li>symmetric。以边缘为中心做镜面对称，如 [1, 2, 3, 4] 边缘扩充一个值，结果为 [1, 1, 2, 3, 4, 4]。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">RandomCrop(size, padding=<span class="hljs-literal">None</span>, pad_if_needed=<span class="hljs-literal">False</span>, fill=<span class="hljs-number">0</span>, padding_mode=<span class="hljs-string">&#x27;constant&#x27;</span>)<br></code></pre></td></tr></table></figure>

<h4 id="RandomResizedCrop"><a href="#RandomResizedCrop" class="headerlink" title="RandomResizedCrop"></a>RandomResizedCrop</h4><p>随机长宽比裁剪，之后再将图片 resize 到给定的 size。</p>
<ul>
<li>size：可以是 tuple 也可以是整数。<code>int</code> 不支持，要用长度为1的序列 [size, ]。</li>
<li>scale： float 的 tuple。指定随机 crop 的比例区间，如 0.08x~1.0x。</li>
<li>ratio： float的tuple。随机长宽比范围设置，如 0.75~1.33。</li>
<li>interpolation：插值方法。默认为双线性插值。由 <code>torchvision.transforms.InterpolationMode</code> 定义。如输入为 Tensor，只有 <code>InterpolationMode.NEAREST</code>，<code>InterpolationMode.BILINEAR</code> 和 <code>InterpolationMode.BICUBIC</code> 可用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">RandomResizedCrop(size, scale=(<span class="hljs-number">0.08</span>, <span class="hljs-number">1.0</span>), ratio=(<span class="hljs-number">3.</span>/<span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>/<span class="hljs-number">3.</span>), interpolation=InterpolationMode.BILINEAR)<br></code></pre></td></tr></table></figure>

<h4 id="FiveCrop"><a href="#FiveCrop" class="headerlink" title="FiveCrop"></a>FiveCrop</h4><p>将给定图片裁剪成 4 张边角图片和 1 张中心图片。如果输入图片是 <code>torch.Tensor</code>，则 expected shape 为 <code>[d, H, W]</code>，其中 d 为指示维度的数。</p>
<ul>
<li>size：可以是 tuple 也可以是整数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">FiveCrop(size)<br></code></pre></td></tr></table></figure>

<h4 id="TenCrop"><a href="#TenCrop" class="headerlink" title="TenCrop"></a>TenCrop</h4><p>将给定的图片裁剪成 4 张边角图片和 1 张中心图片，然后全部翻转 (默认水平翻转)。如果输入图片是 <code>torch.Tensor</code>，则 expected shape 为 <code>[d, H, W]</code>，其中 d 为指示维度的数。</p>
<ul>
<li>size：可以是 tuple 也可以是整数。</li>
<li>vertical_flip：使用水平翻转或竖直翻转。<code>True</code> 垂直翻转，<code>False</code> 水平翻转。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">TenCrop(size, vertical_flip=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure>

<h3 id="翻转和旋转"><a href="#翻转和旋转" class="headerlink" title="翻转和旋转"></a>翻转和旋转</h3><h4 id="RandomHorizontalFlip"><a href="#RandomHorizontalFlip" class="headerlink" title="RandomHorizontalFlip"></a>RandomHorizontalFlip</h4><p>根据给定概率 p 进行水平翻转。如果输入图片是 <code>torch.Tensor</code>，则 expected shape 为 <code>[d, H, W]</code>，其中 d 为指示维度的数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">RandomHorizontalFlip(p=<span class="hljs-number">0.5</span>)<br></code></pre></td></tr></table></figure>

<h4 id="RandomVerticalFlip"><a href="#RandomVerticalFlip" class="headerlink" title="RandomVerticalFlip"></a>RandomVerticalFlip</h4><p>根据给定概率 p 进行垂直翻转。如果输入图片是 <code>torch.Tensor</code>，则 expected shape 为 <code>[d, H, W]</code>，其中 d 为指示维度的数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">RandomVerticalFlip(p=<span class="hljs-number">0.5</span>)<br></code></pre></td></tr></table></figure>

<h4 id="RandomRotation"><a href="#RandomRotation" class="headerlink" title="RandomRotation"></a>RandomRotation</h4><p>根据角度旋转图片。如果输入图片是 <code>torch.Tensor</code>，则 expected shape 为 <code>[d, H, W]</code>，其中 d 为指示维度的数。</p>
<ul>
<li>degrees：旋转角度。如果不是序列 $(min, max)$，则角度范围为 $(-degrees, +degrees)$</li>
<li>interpolation：插值方法。默认为 <code>NEAREST</code>。由 <code>torchvision.transforms.InterpolationMode</code> 定义。如输入为 Tensor，只有 <code>InterpolationMode.NEAREST</code>，<code>InterpolationMode.BILINEAR</code> 和 <code>InterpolationMode.BICUBIC</code> 可用。</li>
<li>expand：<code>true</code> 则填充到足够覆盖整个旋转后的图像，<code>false</code> 则使输出图像和输入图像大小一致。</li>
<li>fill：旋转后的图像外面的区域填充的值，默认为0。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">RandomRotation(degrees, interpolation=InterpolationMode.NEAREST, expand=<span class="hljs-literal">False</span>, center=<span class="hljs-literal">None</span>, fill=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>

<h3 id="图像变换"><a href="#图像变换" class="headerlink" title="图像变换"></a>图像变换</h3><h4 id="Resize"><a href="#Resize" class="headerlink" title="Resize"></a>Resize</h4><p>把图像 Resize 成给定的 size。</p>
<ul>
<li>size：(sequence or int) 期望输出 size。如果 size&#x3D;(h, w)，则输出降为匹配的 size，如果 size 是一个 <code>int</code>，将会把较小的边设置为该值。也就是说，如果 height &gt; width，那么图片将会 rescale 成 (size * height &#x2F; width, size)。</li>
<li>interpolation：插值方法。默认为 <code>BILINEAR</code>。由 <code>torchvision.transforms.InterpolationMode</code> 定义。如输入为 Tensor，只有 <code>InterpolationMode.NEAREST</code>，<code>InterpolationMode.BILINEAR</code> 和 <code>InterpolationMode.BICUBIC</code> 可用。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Resize(size, interpolation=InterpolationMode.BILINEAR)<br></code></pre></td></tr></table></figure>

<h4 id="Normalize"><a href="#Normalize" class="headerlink" title="Normalize"></a>Normalize</h4><p>用平均值和均方差归一化 Tensor 图像。</p>
<p><em>该方法不能被用于 PIL 图像。</em></p>
<p>给出 n 个 channel 的 <code>(mean[1],...,mean[n]) </code> 和 <code>(std[1],..,std[n])</code> ，该方法将依次归一化每个 channel，也就是 <code>output[channel] = (input[channel] - mean[channel]) / std[channel]</code> 。</p>
<ul>
<li>mean：每个 channel 的均值。</li>
<li>std：每个 channel 的均方差。</li>
<li>inplace：是否采用 <code>in-place</code> 。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Normalize(mean, std, inplace=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure>

<h4 id="ToTensor"><a href="#ToTensor" class="headerlink" title="ToTensor"></a>ToTensor</h4><p>将 <code>PIL Image</code> 或者 <code>numpy.ndarray</code> 转换为 <code>tensor</code> 。</p>
<p>将 [0, 255] 范围内的 $(H\times W\times C)$ 转换为 [0.0, 1.0] 范围内的 $(C\times H\times W)$ <code>torch.FloatTensor</code>。</p>
<p><code>PIL Image</code> 应该是 (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) 之中的一种模式， <code>numpy.ndarray</code> 的 <code>dtype = np.uint8</code> 。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">ToTensor(pic)<br></code></pre></td></tr></table></figure>

<h4 id="ToPILImage"><a href="#ToPILImage" class="headerlink" title="ToPILImage"></a>ToPILImage</h4><p>将 <code>tensor</code> 或者 <code>ndarray</code> 转换成 <code>PIL Image</code>。</p>
<p>将 $(C\times H\times W)$ 的 <code>Tensor</code> 转换为 $(H\times W\times C)$ 的 <code>PIL Image</code>。、</p>
<p><code>mode</code> 可以参考 <a target="_blank" rel="noopener" href="https://pillow.readthedocs.io/en/latest/handbook/concepts.html#concept-modes">Pillow 官网</a> 上的色彩空间。</p>
<ul>
<li>mode：<code>PIL Image</code> 的色彩空间，如果 <code>mode=None</code>，则会根据输入数据进行适配<ul>
<li><code>input channel=4</code> ，<code>mode=RGBA</code></li>
<li><code>input channel=3</code> ，<code>mode=RGB</code></li>
<li><code>input channel=2</code> ，<code>mode=LA</code></li>
<li><code>input channel=1</code> ，<code>mode</code> 由数据类型，也就是 <code>int</code>，<code>float</code>，<code>short</code> 决定。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">convert = ToPILImage(mode=<span class="hljs-literal">None</span>)<br>output = convert(pic)<br></code></pre></td></tr></table></figure>

<h4 id="Pad"><a href="#Pad" class="headerlink" title="Pad"></a>Pad</h4><p>使用给定的 <code>padding</code> 值填充给定的图像。</p>
<p>如果 image 是 <code>Tensor</code> ，它的大小应该是 $[d, H, W]$。对于 <code>reflect</code> 和 <code>symmetric</code> mode来说，最多 2 维，对于 <code>edge</code> mode 来说，最多 3 维，对于 <code>constant</code> mode 来说，可以是任意维度。</p>
<ul>
<li>padding：设置填充多少个 pixel。当为 <code>int</code> 时，图像上下左右均填充 <code>int</code> 个，若有两个数，则第一个数为左右扩充多少，第二个数表示上下的。当有 4 个数时，则为 左、上、右、下 各填充多少个。</li>
<li>fill：填充的值 (仅当填充 mode 为 ‘constant’ 时有效。当为 <code>int</code> 时，各通道均填充该值，当为 [r, g, b] tuple 时，表示RGB通道分别填充的值。</li>
<li>padding_mode：有 4 种填充模式：<ul>
<li>constant 常量。</li>
<li>edge 按照图片边缘像素值填充。</li>
<li>reflect 。以边缘值为中心做镜面对称，如 [1, 2, 3, 4] 边缘扩充一个值，结果为 [2, 1, 2, 3, 4, 3]。</li>
<li>symmetric。以边缘为中心做镜面对称，如 [1, 2, 3, 4] 边缘扩充一个值，结果为 [1, 1, 2, 3, 4, 4]。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Pad(padding, fill=<span class="hljs-number">0</span>, padding_mode=<span class="hljs-string">&#x27;constant&#x27;</span>)<br></code></pre></td></tr></table></figure>

<h4 id="ColorJitter"><a href="#ColorJitter" class="headerlink" title="ColorJitter"></a>ColorJitter</h4><p>修改亮度，对比度，饱和度还有色调。</p>
<p>如果 image 是 <code>Tensor</code>，它的大小应该是 $[\dots, 3, H, W]$，$\dots$ 是维度。</p>
<ul>
<li>brightness：浮点数 或 (min, max) 的 tuple，亮度的抖动。brightness_factor 从 [max(0, 1 - brightness), 1 + brightness] 或者 [min, max] 中均匀选择。应该是非负数。</li>
<li>contrast：浮点数 或 (min, max) 的 tuple，对比度的抖动。contrast_factor 从 [max(0, 1 - contrast), 1 + contrast] 或者 [min, max] 中均匀选择。应该是非负数。</li>
<li>saturation：浮点数 或 (min, max) 的 tuple，饱和度的抖动。saturation_factor 从 [max(0, 1 - saturation), 1 + saturation] 或者 [min, max] 中均匀选择。应该是非负数。</li>
<li>hue：浮点数 或 (min, max) 的 tuple，色调的抖动。hue_factor 从 [-hue, hue] 或者 [min, max] 中均匀选择。应该 <code>0&lt;= hue &lt;= 0.5</code> 或者 <code>-0.5&lt;= min &lt;= max &lt;=0.5</code> 。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">ColorJitter(brightness=<span class="hljs-number">0</span>, contrast=<span class="hljs-number">0</span>, saturation=<span class="hljs-number">0</span>, hue=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>

<h4 id="Grayscale"><a href="#Grayscale" class="headerlink" title="Grayscale"></a>Grayscale</h4><p>将图片转换为灰度图。</p>
<p>如果 image 是 <code>Tensor</code>，它的大小应该是 $[\dots,3,H,W]$，$\dots$ 是维度。</p>
<ul>
<li>num_output_channels：(1 或者 3) 输出图像的通道数量。<ul>
<li><code>num_output_channels == 1</code>：返回单通道图像。</li>
<li><code>num_output_channels == 3</code>：返回 <code>r==g==b</code> 的 3 通道图像。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Grayscale(num_output_channels=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<h4 id="RandomGrayscale"><a href="#RandomGrayscale" class="headerlink" title="RandomGrayscale"></a>RandomGrayscale</h4><p>根据概率 p 将图片转为灰度图。</p>
<p>如果 image 是 <code>Tensor</code>，它的大小应该是 $[\dots,3,H,W]$，$\dots$ 是维度。</p>
<p>如果输入图像为单通道，则返回单通道，如果为 3 通道，则返回 <code>r==g==b</code> 的 3 通道灰度图。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">RandomGrayscale(p=<span class="hljs-number">0.1</span>)<br></code></pre></td></tr></table></figure>

<h4 id="LinearTransformation"><a href="#LinearTransformation" class="headerlink" title="LinearTransformation"></a>LinearTransformation</h4><p>用方阵和偏移量的 mean_vector 来做线性变换，可用于白化处理。</p>
<p>给定 <code>transformation_matrix</code> 和 <code>mean_vector</code>，会先将 <code>Tensor</code> 展平并从中减去 <code>mean_vector</code> ，然后用 <code>transformation_matrix</code> 计算点积，最后将 <code>Tensor</code> 重塑为之前的 shape。</p>
<ul>
<li>transformation_matrix：$[D\times D]$ 的 <code>Tensor</code>，$D &#x3D; C\times H\times W$。</li>
<li>mean_vector：$[D]$ 的 <code>Tensor</code>，$D&#x3D;C\times H\times W$。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">LinearTransformation(transformation_matrix, mean_vector)<br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">Applications:</span><br><span class="hljs-string">	whitening transformation: Suppose X is a column vector zero-centered data.</span><br><span class="hljs-string">	Then compute the data covariance matrix [D x D] with torch.mm(X.t(), X),</span><br><span class="hljs-string">	perform SVD on this matrix and pass it as transformation_matrix.</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure>

<h4 id="RandomAffine"><a href="#RandomAffine" class="headerlink" title="RandomAffine"></a>RandomAffine</h4><p>图像保持中心不变的随机仿射变换。</p>
<p>如果 image 是 <code>Tensor</code>，它的大小应该是 $[\dots,H,W]$，$\dots$ 是维度。</p>
<ul>
<li>degrees：旋转选择角度的范围。如果角度为一个值，则范围为 (-degrees, +degrees)。tuple 情况下范围为 (min, max)。如果 <code>degrees=0</code>，则停用旋转。</li>
<li>translate：水平和垂直平移的最大绝对比例的 tuple。如 <code>translate=(a, b)</code>，那么水平平移在 <code>-img_width * a &lt; dx &lt; img_width * a</code> 范围内随机选择，垂直平移在 <code>-img_height * b &lt; dy &lt; img_height * b</code> 范围内随机选择。默认则不会偏移。</li>
<li>scale：放缩的范围区间。如 <code>(a, b)</code>，则范围为 <code>a&lt;= scale &lt;= b</code>，默认保持原大小。</li>
<li>shear：错切选择角度的范围。如果 <code>shear</code> 是一个数字，则平行于 x 轴的错切范围为 (-shear, +shear)。如果 <code>shear</code> 是一个包含两个数字的序列，则平行于 x 轴的错切范围为 (shear[0], shear[1])。如果 <code>shear</code> 是一个包含 4 个数字的序列，那么平行于 x 轴的错切范围为 (shear[0], shear[1])，平行于 y 轴的错切范围为 (shear[2], shear[3])。默认不会应用错切。</li>
<li>interpolation：插值方法。默认为 <code>NEAREST</code>。由 <code>torchvision.transforms.InterpolationMode</code> 定义。如输入为 Tensor，<code>InterpolationMode.NEAREST</code> 和 <code>InterpolationMode.BILINEAR</code> 可用。</li>
<li>fill：区域外填充的值，默认是 0 。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">RandomAffine(degrees, translate=<span class="hljs-literal">None</span>, scale=<span class="hljs-literal">None</span>, shear=<span class="hljs-literal">None</span>, interpolation=InterpolationMode.NEAREST, fill=<span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure>

<h3 id="Transform-组合操作"><a href="#Transform-组合操作" class="headerlink" title="Transform 组合操作"></a>Transform 组合操作</h3><h4 id="Compose"><a href="#Compose" class="headerlink" title="Compose"></a>Compose</h4><p>将多个 transform 组合起来使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">transforms.Compose([<br>    transforms.CenterCrop(<span class="hljs-number">10</span>),<br>    transforms.ToTensor(),<br>])<br></code></pre></td></tr></table></figure>

<h4 id="RandomChoice"><a href="#RandomChoice" class="headerlink" title="RandomChoice"></a>RandomChoice</h4><p>应用从列表中随机选择的单个转换。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">transforms.RandomChoice([<br>    transforms.CenterCrop(<span class="hljs-number">10</span>),<br>    transforms.ToTensor(),<br>])<br></code></pre></td></tr></table></figure>

<h4 id="RandomApply"><a href="#RandomApply" class="headerlink" title="RandomApply"></a>RandomApply</h4><p>给一个 transform 加上概率，以一定的概率执行该操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">RandomApply(transforms, p=<span class="hljs-number">0.5</span>)<br></code></pre></td></tr></table></figure>

<h4 id="RandomOrder"><a href="#RandomOrder" class="headerlink" title="RandomOrder"></a>RandomOrder</h4><p>将 transforms 中的操作顺序随机打乱。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">RandomOrder([<br>    transforms.CenterCrop(<span class="hljs-number">10</span>),<br>    transforms.ToTensor(),<br>])<br></code></pre></td></tr></table></figure>

<h4 id="Lambda-Transforms"><a href="#Lambda-Transforms" class="headerlink" title="Lambda Transforms"></a>Lambda Transforms</h4><p>还不了解，待补充。</p>
<h2 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h2><p>神经网络由对数据执行操作的层&#x2F;模块组成。<code>torch.nn</code> 命名空间提供了构建自己的神经网络所需的所有构建块。 PyTorch 中的每个模块都是 <code>nn.Module</code> 的子类。 神经网络是一个模块本身，它由其他模块（层）组成。 这种嵌套结构允许轻松构建和管理复杂的架构。</p>
<h3 id="设置训练设备"><a href="#设置训练设备" class="headerlink" title="设置训练设备"></a>设置训练设备</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span><br></code></pre></td></tr></table></figure>

<h3 id="定义网络类"><a href="#定义网络类" class="headerlink" title="定义网络类"></a>定义网络类</h3><p>我们使用子类 <code>nn.Module</code> 定义自己的神经网络，然后在 <code>__init__</code> 中初始化网络层。每个 <code>nn.Module</code> 子类都在 <code>forward</code> 方法中实现对输入数据的操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NeuralNetwork, self).__init__()<br>        self.linear_relu_stack = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">28</span>*<span class="hljs-number">28</span>, <span class="hljs-number">512</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        output = self.linear_relu_stack(x)<br>        <span class="hljs-keyword">return</span> output<br>    <br><span class="hljs-comment"># 将模型载入到对应设备上</span><br>model = NearalNetwork().to(device)<br><span class="hljs-comment"># print 查看模型</span><br><span class="hljs-built_in">print</span>(model)<br></code></pre></td></tr></table></figure>

<h3 id="模型-Layers"><a href="#模型-Layers" class="headerlink" title="模型 Layers"></a>模型 Layers</h3><h4 id="Containers"><a href="#Containers" class="headerlink" title="Containers"></a>Containers</h4><p><code>torch.nn.Module</code> 是所有网络的基类，我们的模型也应该继承这个类。</p>
<h5 id="模块添加和迭代"><a href="#模块添加和迭代" class="headerlink" title="模块添加和迭代"></a>模块添加和迭代</h5><ul>
<li><p>add_module(name, module)</p>
<p>将一个 <code>child module</code> 添加到当前 <code>module</code> 。被添加的 <code>module</code> 可以通过 <code>name</code> 属性获取。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Model, self).__init__()<br>        self.add_module(<span class="hljs-string">&quot;conv&quot;</span>, nn.Conv2d(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">4</span>))<br>        <span class="hljs-comment">#self.conv = nn.Conv2d(10, 20, 4) 和上面这个增加module的方式等价</span><br>model = Model()<br><span class="hljs-built_in">print</span>(model.conv)<br></code></pre></td></tr></table></figure>
</li>
<li><p>modules()</p>
<p>返回一个包含 当前模型 所有模块的迭代器。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Model, self).__init__()<br>        self.add_module(<span class="hljs-string">&quot;conv&quot;</span>, nn.Conv2d(<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">4</span>))<br>        self.add_module(<span class="hljs-string">&quot;conv1&quot;</span>, nn.Conv2d(<span class="hljs-number">20</span> ,<span class="hljs-number">10</span>, <span class="hljs-number">4</span>))<br>model = Model()<br><br><span class="hljs-keyword">for</span> module <span class="hljs-keyword">in</span> model.modules():<br>    <span class="hljs-built_in">print</span>(module)<br></code></pre></td></tr></table></figure>
</li>
<li><p>children()</p>
<p>返回当前模型 <strong>子模块</strong> 的迭代器。</p>
</li>
<li><p>named_children()</p>
<p>返回 包含 模型当前子模块 的迭代器，包含模块名字和模块本身。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> name, module <span class="hljs-keyword">in</span> model.named_children():<br>    <span class="hljs-keyword">if</span> name <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;conv4&#x27;</span>, <span class="hljs-string">&#x27;conv5&#x27;</span>]:<br>        <span class="hljs-built_in">print</span>(module)<br></code></pre></td></tr></table></figure></li>
</ul>
<h5 id="模块设备管理"><a href="#模块设备管理" class="headerlink" title="模块设备管理"></a>模块设备管理</h5><ul>
<li><p>cpu(device_id&#x3D;None)</p>
<p>将所有的模型参数 <code>parameters</code> 和 <code>buffers</code> 复制到 CPU。</p>
</li>
<li><p>cuda(device_id&#x3D;None)</p>
<p>将所有的模型参数 <code>parameters</code> 和 <code>buffers</code> 复制给 GPU。</p>
</li>
</ul>
<h5 id="模块数据类型"><a href="#模块数据类型" class="headerlink" title="模块数据类型"></a>模块数据类型</h5><ul>
<li><p>double()</p>
<p>将 <code>parameters</code> 和 <code>buffers</code> 的数据类型转换为 <code>double</code>。</p>
</li>
<li><p>float()</p>
<p>将 <code>parameters</code> 和 <code>buffers</code> 的数据类型转换为 <code>float</code>。</p>
</li>
<li><p>half()</p>
<p>将 <code>parameters</code> 和 <code>buffers</code> 的数据类型转换为 <code>half</code>。</p>
</li>
</ul>
<h5 id="字典查看和导入"><a href="#字典查看和导入" class="headerlink" title="字典查看和导入"></a>字典查看和导入</h5><ul>
<li><p>state_dict()</p>
<p>返回一个字典，保存着<code>module</code>的所有状态（<code>state</code>）。</p>
<p><code>parameters</code> 和 <code>persistent buffers</code> 都会包含在字典中，字典的 <code>key</code> 就是 <code>parameter</code> 和 <code>buffer </code> 的 <code>names </code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Model, self).__init__()<br>        self.conv2 = nn.Linear(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        self.vari = Variable(torch.rand([<span class="hljs-number">1</span>]))<br>        self.par = nn.Parameter(torch.rand([<span class="hljs-number">1</span>]))<br>        self.register_buffer(<span class="hljs-string">&quot;buffer&quot;</span>, torch.randn([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]))<br><br>model = Model()<br><span class="hljs-built_in">print</span>(model.state_dict().keys())<br></code></pre></td></tr></table></figure>
</li>
<li><p>load_state_dict(state_dict)</p>
<p>将 <code>state_dict</code> 中的 <code>parameters</code> 和 <code>buffers</code> 复制到此 <code>module</code>和它的后代中。<code>state_dict</code> 中的 <code>key</code> 必须和 <code>model.state_dict()</code> 返回的 <code>key</code> 一致。</p>
</li>
</ul>
<h5 id="网络训练，梯度，前向传播"><a href="#网络训练，梯度，前向传播" class="headerlink" title="网络训练，梯度，前向传播"></a>网络训练，梯度，前向传播</h5><ul>
<li><p>forward(*input)</p>
<p>定义了每次执行的 计算步骤。 在所有的子类中都需要重写这个函数。</p>
</li>
<li><p>train(mode&#x3D;True)</p>
<p>将 <code>module</code> 设置为 <code>training mode</code>。仅仅当模型中有 <code>Dropout</code> 和 <code>BatchNorm</code> 时才会有影响。</p>
</li>
<li><p>zero_grad()</p>
<p>将 <code>module</code> 中的所有模型参数的梯度设置为0.</p>
</li>
</ul>
<h5 id="容器类-Sequential-ModuleList-ParameterList"><a href="#容器类-Sequential-ModuleList-ParameterList" class="headerlink" title="容器类 Sequential, ModuleList, ParameterList"></a>容器类 Sequential, ModuleList, ParameterList</h5><ul>
<li><p>torch.nn.Sequential(* args)</p>
<p>一个时序容器。<code>Modules</code> 会以他们传入的顺序被添加到容器中。当然，也可以传入一个<code>OrderedDict</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Example of using Sequential</span><br><br>model = nn.Sequential(<br>          nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>),<br>          nn.ReLU(),<br>          nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>),<br>          nn.ReLU()<br>        )<br><span class="hljs-comment"># Example of using Sequential with OrderedDict</span><br>model = nn.Sequential(OrderedDict([<br>          (<span class="hljs-string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>)),<br>          (<span class="hljs-string">&#x27;relu1&#x27;</span>, nn.ReLU()),<br>          (<span class="hljs-string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>)),<br>          (<span class="hljs-string">&#x27;relu2&#x27;</span>, nn.ReLU())<br>        ]))<br></code></pre></td></tr></table></figure>
</li>
<li><p>torch.nn.ModuleList(modules&#x3D;None)</p>
<p>将 <code>submodules</code> 保存在一个 <code>list</code> 中。</p>
<p><code>ModuleList</code> 可以像一般的 <code>Python list</code> 一样被<code>索引</code>。而且 <code>ModuleList</code> 中包含的<code>modules</code> 已经被正确的注册，对所有的 <code>module method</code> 可见。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModule</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModule, self).__init__()<br>        self.linears = nn.ModuleList([nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># ModuleList can act as an iterable, or be indexed         using ints</span><br>        <span class="hljs-keyword">for</span> i, l <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.linears):<br>            x = self.linears[i // <span class="hljs-number">2</span>](x) + l(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>

<ul>
<li><p>append(module)</p>
<p>等价于 list 的 <code>append() </code> 。</p>
</li>
</ul>
</li>
<li><p>torch.nn.ParameterList(parameters&#x3D;None)</p>
<p>将 <code>submodules</code> 保存在一个 <code>list</code> 中。</p>
<p><code>ParameterList</code> 可以像一般的 <code>Python list</code> 一样被<code>索引</code>。而且 <code>ParameterList</code> 中包含的 <code>parameters</code> 已经被正确的注册，对所有的 <code>module method</code> 可见。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModule</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModule, self).__init__()<br>        self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>)])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># ModuleList can act as an iterable, or be indexed using ints</span><br>        <span class="hljs-keyword">for</span> i, p <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.params):<br>            x = self.params[i // <span class="hljs-number">2</span>].mm(x) + p.mm(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure>

<ul>
<li><p>append(parameter)</p>
<p>等价于 list 的 <code>append() </code> 。</p>
</li>
</ul>
</li>
</ul>
<h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><h5 id="X维卷积"><a href="#X维卷积" class="headerlink" title="X维卷积"></a>X维卷积</h5><p>卷积操作。</p>
<p>$N$ 为 <code>batch size</code> ，$C$ 为 <code>channel</code> 数量，$L$ 为信号序列的长度。</p>
<p>可使用的为 <code>1-3</code> 维的卷积层，函数如下：</p>
<ul>
<li><p><code>torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</code></p>
<ul>
<li><p>输入: $(N,C_{in},L_{in})$</p>
</li>
<li><p>输出: $(N,C_{out},L_{out})$</p>
</li>
<li><p>$L_{out}$ 的 Shape 为：</p>
<p>$$L_{out}&#x3D;floor((L_{in}+2<em>padding-dilation</em>(kernerl_size-1)-1)&#x2F;stride+1)$$</p>
</li>
</ul>
</li>
<li><p><code>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</code></p>
<ul>
<li><p>输入: $(N,C_{in},H_{in},W_{in})$</p>
</li>
<li><p>输出: $(N,C_{out},H_{out},W_{out})$</p>
</li>
<li><p>$H_{out}$ 和 $W_{out}$ 的 Shape 为：</p>
<p>$$H_{out}&#x3D;floor((H_{in}+2<em>padding[0]-dilation[0]</em>(kernerl_size[0]-1)-1)&#x2F;stride[0]+1)$$</p>
<p>$$W_{out}&#x3D;floor((W_{in}+2<em>padding[1]-dilation[1]</em>(kernerl_size[1]-1)-1)&#x2F;stride[1]+1)$$</p>
</li>
</ul>
</li>
<li><p><code>torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</code></p>
<ul>
<li><p>输入: $(N,C_{in},D_{in},H_{in},W_{in})$</p>
</li>
<li><p>输出: $(N,C_{out},D_{out},H_{out},W_{out})$</p>
</li>
<li><p>$D_{out}, H_{out}, W_{out}$ 的 Shape 为：</p>
<p>$$D_{out}&#x3D;floor((D_{in}+2<em>padding[0]-dilation[0]</em>(kernerl_size[0]-1)-1)&#x2F;stride[0]+1)$$</p>
<p>$$H_{out}&#x3D;floor((H_{in}+2<em>padding[1]-dilation[1]</em>(kernerl_size[1]-1)-1)&#x2F;stride[1]+1)$$</p>
<p>$$W_{out}&#x3D;floor((W_{in}+2<em>padding[2]-dilation[2]</em>(kernerl_size[2]-1)-1)&#x2F;stride[2]+1)$$</p>
</li>
</ul>
</li>
</ul>
<p><strong>输入输出的计算方式：</strong></p>
<p>$$ out(N_i, C_{out_j})&#x3D;bias(C_{out_j})+\sum^{C_{in}-1}<em>{k&#x3D;0}weight(C</em>{out_j},k)\bigotimes input(N_i,k) $$</p>
<p>其中 $\bigotimes$ 为 <code>cross-correlation</code> 操作符。</p>
<p><strong>参数及其意义为：</strong></p>
<ul>
<li>in_channels(<code>int</code>) – 输入信号的通道</li>
<li>out_channels(<code>int</code>) – 卷积产生的通道</li>
<li>kerner_size(<code>int</code> or <code>tuple</code>) - 卷积核的尺寸</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 卷积步长</li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) – 卷积核元素之间的间距</li>
<li>groups(<code>int</code>, <code>optional</code>) – 从输入通道到输出通道的阻塞连接数<ul>
<li><code>groups=1</code>，所有输出由输入卷积得到</li>
<li><code>groups=2</code>，将输入 <code>channel</code> 平分成两份，卷积后进行 <code>concat</code> 操作</li>
<li><code>groups=in_channels</code>，所有 <code>input_channel</code> 都由自己的滤波器进行卷积，大小为 <code>out_channels/in_channels</code></li>
</ul>
</li>
<li>bias(<code>bool</code>, <code>optional</code>) - 如果<code>bias=True</code>，添加偏置</li>
</ul>
<h5 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h5><p>反卷积操作，相当于使用空洞卷积进行上采样。</p>
<p>$N$ 为 <code>batch size</code> ，$C$ 为 <code>channel</code> 数量，$L$ 为信号序列的长度。</p>
<p>可使用的为 <code>1-3</code> 维的卷积层，函数如下：</p>
<ul>
<li><p><code>torch.nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)</code></p>
<ul>
<li><p>输入: $(N,C_{in},L_{in})$</p>
</li>
<li><p>输出: $(N,C_{out},L_{out})$</p>
</li>
<li><p>$L_{out}$ 的 Shape 为：</p>
<p>$$L_{out}&#x3D;(L_{in}-1)<em>stride-2</em>padding+dilation\times(kernel_size-1)+output_padding+1$$</p>
</li>
</ul>
</li>
<li><p><code>torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)</code></p>
<ul>
<li><p>输入: $(N,C_{in},H_{in},W_{in})$</p>
</li>
<li><p>输出: $(N,C_{out},H_{out},W_{out})$</p>
</li>
<li><p>$H_{out}$ 和 $W_{out}$ 的 Shape 为：</p>
<p>$$H_{out}&#x3D;(H_{in}-1)<em>stride[0]-2</em>padding[0]+dilation[0]\times (kernel_size[0]-1)+output_padding[0]+1$$</p>
<p>$$W_{out}&#x3D;(W_{in}-1)<em>stride[1]-2</em>padding[1]+dilation[1]\times(kernel_size[1]-1)+output_padding[1]+1$$</p>
</li>
</ul>
</li>
<li><p><code>torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)</code></p>
<ul>
<li><p>输入: $(N,C_{in},D_{in},H_{in},W_{in})$</p>
</li>
<li><p>输出: $(N,C_{out},D_{out},H_{out},W_{out})$</p>
</li>
<li><p>$D_{out}, H_{out}, W_{out}$ 的 Shape 为：</p>
<p>$$D_{out}&#x3D;(D_{in}-1)<em>stride[0]-2</em>padding[0]+dilation[0]\times(kernel_size[0]-1)+output_padding[0]+1$$</p>
<p>$$H_{out}&#x3D;(H_{in}-1)<em>stride[1]-2</em>padding[1]+dilation[1]\times(kernel_size[1]-1)+output_padding[1]+1$$</p>
<p>$$W_{out}&#x3D;(W_{in}-1)<em>stride[2]-2</em>padding[2]+dilation[2]\times(kernel_size[2]-1)+output_padding[2]+1$$</p>
</li>
</ul>
</li>
</ul>
<h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><h5 id="Maxpool"><a href="#Maxpool" class="headerlink" title="Maxpool"></a>Maxpool</h5><p>$N$ 为 <code>batch size</code> ，$C$ 为 <code>channel</code> 数量。</p>
<p>可使用的为 <code>1-3</code> 维的池化层，函数如下：</p>
<ul>
<li><p><code>torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</code></p>
<ul>
<li><p>输入: $(N,C_{in},L_{in})$</p>
</li>
<li><p>输出: $(N,C_{out},L_{out})$</p>
</li>
<li><p>$L_{out}$ 的 Shape 为：</p>
<p>$$L_{out}&#x3D;floor((L_{in} + 2<em>padding - dilation</em>(kernel_size - 1) - 1)&#x2F;stride + 1)$$</p>
</li>
</ul>
</li>
<li><p><code>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</code></p>
<ul>
<li><p>输入: $(N,C_{in},H_{in},W_{in})$</p>
</li>
<li><p>输出: $(N,C_{out},H_{out},W_{out})$</p>
</li>
<li><p>$H_{out}$ 和 $W_{out}$ 的 Shape 为：</p>
<p>$$H_{out}&#x3D;floor((H_{in} + 2<em>padding[0] - dilation[0]</em>(kernel_size[0] - 1) - 1)&#x2F;stride[0] + 1)$$</p>
<p>$$W_{out}&#x3D;floor((W_{in} + 2<em>padding[1] - dilation[1]</em>(kernel_size[1] - 1) - 1)&#x2F;stride[1] + 1)$$</p>
</li>
</ul>
</li>
<li><p><code>torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</code></p>
<ul>
<li><p>输入: $(N,C_{in},D_{in},H_{in},W_{in})$</p>
</li>
<li><p>输出: $(N,C_{out},D_{out},H_{out},W_{out})$</p>
</li>
<li><p>$D_{out}, H_{out}, W_{out}$ 的 Shape 为：</p>
<p>$$D_{out}&#x3D;floor((D_{in} + 2<em>padding[0] - dilation[0]</em>(kernel_size[0] - 1) - 1)&#x2F;stride[0] + 1)$$</p>
<p>$$H_{out}&#x3D;floor((H_{in} + 2<em>padding[1] - dilation[1]</em>(kernel_size[1] - 1) - 1)&#x2F;stride[1] + 1)$$</p>
<p>$$W_{out}&#x3D;floor((W_{in} + 2<em>padding[2] - dilation[2]</em>(kernel_size[2] - 1) - 1)&#x2F;stride[2] + 1)$$</p>
</li>
</ul>
</li>
</ul>
<h5 id="MaxUnpool"><a href="#MaxUnpool" class="headerlink" title="MaxUnpool"></a>MaxUnpool</h5><p><code>Maxpool</code>的逆过程，不过并不是完全的逆过程，因为在<code>maxpool</code>的过程中，一些值已经丢失。 <code>MaxUnpool</code>输入<code>MaxPool</code>的输出，包括最大值的索引，并计算所有<code>maxpool</code>过程中非最大值被设置为零的部分的反向。</p>
<ul>
<li><code>torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)</code></li>
<li><code>torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)</code></li>
<li><code>torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)</code></li>
</ul>
<h5 id="AvgPool"><a href="#AvgPool" class="headerlink" title="AvgPool"></a>AvgPool</h5><p>平均池化。</p>
<ul>
<li><code>torch.nn.AvgPool1d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)</code></li>
<li><code>torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)</code></li>
<li><code>torch.nn.AvgPool3d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)</code></li>
</ul>
<p><strong>参数：</strong></p>
<ul>
<li>kernel_size(<code>int</code> or <code>tuple</code>) - 池化窗口大小</li>
<li>stride(<code>int</code> or <code>tuple</code>, <code>optional</code>) - max pooling的窗口移动的步长。默认值是<code>kernel_size</code></li>
<li>padding(<code>int</code> or <code>tuple</code>, <code>optional</code>) - 输入的每一条边补充0的层数</li>
<li>dilation(<code>int</code> or <code>tuple</code>, <code>optional</code>) – 一个控制窗口中元素步幅的参数</li>
<li>ceil_mode - 如果等于<code>True</code>，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作</li>
<li>count_include_pad - 如果等于<code>True</code>，计算平均池化时，将包括<code>padding</code>填充的0</li>
</ul>
<h4 id="非线性激活层"><a href="#非线性激活层" class="headerlink" title="非线性激活层"></a>非线性激活层</h4><ul>
<li><p><code>torch.nn.ReLU(inplace=False) </code></p>
<p>表达式：${ReLU}(x)&#x3D; max(0, x)$</p>
</li>
<li><p><code>torch.nn.Sigmoid()</code></p>
<p>表达式：$f(x)&#x3D;1&#x2F;(1+e^{-x})$</p>
</li>
<li><p><code>torch.nn.Tanh()</code></p>
<p>表达式：$f(x)&#x3D;\frac{exp(x)-exp(-x)}{exp(x)+exp(-x)}$</p>
</li>
<li><p><code>torch.nn.Softmax()</code></p>
<p>表达式：$f(x_i)&#x3D;\frac{exp(x_i)}{\sum_j exp(x_j)}$</p>
</li>
<li><p><code>torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)</code></p>
<p>表达式：$f(x) &#x3D; max(0, x) + {negative_slope} * min(0, x)$</p>
<ul>
<li>negative_slope：控制负斜率的角度，默认等于0.01</li>
</ul>
</li>
<li><p><code>torch.nn.ELU(alpha=1.0, inplace=False)</code></p>
<p>表达式：$f(x) &#x3D; max(0,x) + min(0, alpha * (e^x - 1))$</p>
</li>
<li><p><code>torch.nn.Threshold(threshold, value, inplace=False)</code></p>
<p>表达式：$y&#x3D;x, if ,,, x&gt;&#x3D;threshold;,, y&#x3D;value, if,,,x&lt;threshold$</p>
<ul>
<li>threshold：阈值</li>
<li>value：输入值小于阈值则会被value代替</li>
</ul>
</li>
</ul>
<h4 id="Normalization-层"><a href="#Normalization-层" class="headerlink" title="Normalization 层"></a>Normalization 层</h4><h5 id="BatchNorm"><a href="#BatchNorm" class="headerlink" title="BatchNorm"></a>BatchNorm</h5><p>对 mini-batch 的输入进行 Batch Normalization 操作：</p>
<p>$$y&#x3D;\frac{x-mean[x]}{\sqrt{Var[x]}+\epsilon}\times gamma+\beta$$</p>
<p>在每一个小批量（mini-batch）数据中，计算输入各个维度的均值和标准差。gamma与beta是可学习的大小为C的参数向量（C为输入大小）</p>
<p>在训练时，该层计算每次输入的均值与方差，并进行移动平均。移动平均默认的动量值为0.1。</p>
<p>在验证时，训练求得的均值&#x2F;方差将用于标准化验证数据。</p>
<ul>
<li><code>torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True)</code></li>
<li><code>torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)</code></li>
<li><code>torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True)</code></li>
</ul>
<p><strong>参数：</strong></p>
<ul>
<li><strong>num_features：</strong> 来自期望输入的特征数。</li>
<li><strong>eps：</strong> 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。</li>
<li><strong>momentum：</strong> 动态均值和动态方差所使用的动量。默认为0.1。</li>
<li><strong>affine：</strong> 一个布尔值，当设为true，给该层添加可学习的仿射变换参数。</li>
</ul>
<h4 id="Recurrent-层"><a href="#Recurrent-层" class="headerlink" title="Recurrent 层"></a>Recurrent 层</h4><h5 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h5><p><code>torch.nn.RNN( *args,* * kwargs)</code></p>
<p>将一个多层的 <code>Elman RNN</code>，激活函数为 <code>tanh</code> 或者 <code>ReLU</code>，用于输入序列。</p>
<p>对输入序列中每个元素，<code>RNN</code>每层的计算公式为 $$ h_t&#x3D;tanh(W_{ih}x_t+b_{ih}+W_{hh}h_{t-1}+b_{hh}) $$ $h_t$是时刻$t$的隐状态。 $x_t$是上一层时刻$t$的隐状态，或者是第一层在时刻$t$的输入。如果 <code>nonlinearity=&#39;relu&#39;</code> ,那么将使用 <code>relu</code> 代替 <code>tanh</code> 作为激活函数。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input_size – 输入<code>x</code>的特征数量。</li>
<li>hidden_size – 隐层的特征数量。</li>
<li>num_layers – RNN的层数。</li>
<li>nonlinearity – 指定非线性函数使用 <code>tanh</code> 还是 <code>relu</code>。默认是 <code>tanh</code>。</li>
<li>bias – 如果是 <code>False</code>，那么RNN层就不会使用偏置权重 $b_ih$和$b_hh$,默认是 <code>True</code></li>
<li>batch_first – 如果 <code>True</code> 的话，那么输入 <code>Tensor</code> 的shape应该是[batch_size, time_step, feature],输出也是这样。</li>
<li>dropout – 如果值非零，那么除了最后一层外，其它层的输出都会套上一个 <code>dropout</code> 层。</li>
<li>bidirectional – 如果 <code>True</code>，将会变成一个双向 <code>RNN</code>，默认为 <code>False</code>。</li>
</ul>
<h5 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h5><p><code>torch.nn.LSTM( *args,* * kwargs)</code></p>
<p>将一个多层的 <code>(LSTM)</code> 应用到输入序列。</p>
<p>对输入序列的每个元素，<code>LSTM</code>的每层都会执行以下计算：<br>$$<br>\begin{aligned} i_t &amp;&#x3D;sigmoid(W_{ii}x_t+b_{ii}+W_{hi}h_{t-1}+b_{hi}) \ f_t &amp;&#x3D; sigmoid(W_{if}x_t+b_{if}+W_{hf}h_{t-1}+b_{hf}) \ o_t &amp;&#x3D; sigmoid(W_{io}x_t+b_{io}+W_{ho}h_{t-1}+b_{ho})\ g_t &amp;&#x3D; tanh(W_{ig}x_t+b_{ig}+W_{hg}h_{t-1}+b_{hg})\ c_t &amp;&#x3D; f_t\odot c_{t-1}+i_t\odot g_t\ h_t &amp;&#x3D; o_t\odot tanh(c_t) \end{aligned}<br>$$</p>
<p>$h_t$是时刻$t$的隐状态，$c_t$是时刻$t$的细胞状态，$x_t$是上一层的在时刻$t$的隐状态或者是第一层在时刻$t$的输入。$i_t, f_t, g_t, o_t$ 分别代表 输入门，遗忘门，细胞和输出门。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input_size – 输入的特征维度</li>
<li>hidden_size – 隐状态的特征维度</li>
<li>num_layers – 层数（和时序展开要区分开）</li>
<li>bias – 如果为<code>False</code>，那么<code>LSTM</code>将不会使用$b_{ih},b_{hh}$，默认为<code>True</code>。</li>
<li>batch_first – 如果为<code>True</code>，那么输入和输出<code>Tensor</code>的形状为<code>(batch, seq, feature)</code></li>
<li>dropout – 如果非零的话，将会在<code>RNN</code>的输出上加个<code>dropout</code>，最后一层除外。</li>
<li>bidirectional – 如果为<code>True</code>，将会变成一个双向<code>RNN</code>，默认为<code>False</code>。</li>
</ul>
<h4 id="Transformer-层"><a href="#Transformer-层" class="headerlink" title="Transformer 层"></a>Transformer 层</h4><h5 id="nn-Transformer"><a href="#nn-Transformer" class="headerlink" title="nn.Transformer"></a>nn.Transformer</h5><p>一个基于 “Attension is All You Need” 实现的 Transformer 模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.nn.Transformer(d_model=<span class="hljs-number">512</span>, nhead=<span class="hljs-number">8</span>, num_encoder_layers=<span class="hljs-number">6</span>, num_decoder_layers=<span class="hljs-number">6</span>, dim_feedforward=<span class="hljs-number">2048</span>, dropout=<span class="hljs-number">0.1</span>, activation=&lt;function relu&gt;, custom_encoder=<span class="hljs-literal">None</span>, custom_decoder=<span class="hljs-literal">None</span>, layer_norm_eps=<span class="hljs-number">1e-05</span>, batch_first=<span class="hljs-literal">False</span>, norm_first=<span class="hljs-literal">False</span>, device=<span class="hljs-literal">None</span>, dtype=<span class="hljs-literal">None</span>)<br></code></pre></td></tr></table></figure>

<p><strong>参数：</strong></p>
<ul>
<li><strong>d_model</strong> – the number of expected features in the encoder&#x2F;decoder inputs (default&#x3D;512).</li>
<li><strong>nhead</strong> – the number of heads in the multiheadattention models (default&#x3D;8).</li>
<li><strong>num_encoder_layers</strong> – the number of sub-encoder-layers in the encoder (default&#x3D;6).</li>
<li><strong>num_decoder_layers</strong> – the number of sub-decoder-layers in the decoder (default&#x3D;6).</li>
<li><strong>dim_feedforward</strong> – the dimension of the feedforward network model (default&#x3D;2048).</li>
<li><strong>dropout</strong> – the dropout value (default&#x3D;0.1).</li>
<li><strong>activation</strong> – the activation function of encoder&#x2F;decoder intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. Default: relu</li>
<li><strong>custom_encoder</strong> – custom encoder (default&#x3D;None).</li>
<li><strong>custom_decoder</strong> – custom decoder (default&#x3D;None).</li>
<li><strong>layer_norm_eps</strong> – the eps value in layer normalization components (default&#x3D;1e-5).</li>
<li><strong>batch_first</strong> – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature). Default: <code>False</code> (seq, batch, feature).</li>
<li><strong>norm_first</strong> – if <code>True</code>, encoder and decoder layers will perform LayerNorms before other attention and feedforward operations, otherwise after. Default: <code>False</code> (after).</li>
</ul>
<h5 id="nn-TransformerEncoder"><a href="#nn-TransformerEncoder" class="headerlink" title="nn.TransformerEncoder"></a>nn.TransformerEncoder</h5><p>由 N 个 encoder 层堆叠形成的 TransformerEncoder。</p>
<p><code>torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None)</code></p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>encoder_layer</strong> – an instance of the TransformerEncoderLayer() class (required).</li>
<li><strong>num_layers</strong> – the number of sub-encoder-layers in the encoder (required).</li>
<li><strong>norm</strong> – the layer normalization component (optional).</li>
</ul>
<h5 id="nn-TransformerDecoder"><a href="#nn-TransformerDecoder" class="headerlink" title="nn.TransformerDecoder"></a>nn.TransformerDecoder</h5><p>由 N 个 decoder 层堆叠形成的 TransformerDecoder。</p>
<p><code>torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)</code></p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>decoder_layer</strong> – an instance of the TransformerDecoderLayer() class (required).</li>
<li><strong>num_layers</strong> – the number of sub-decoder-layers in the decoder (required).</li>
<li><strong>norm</strong> – the layer normalization component (optional).</li>
</ul>
<h4 id="Linear-层"><a href="#Linear-层" class="headerlink" title="Linear 层"></a>Linear 层</h4><h5 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h5><p><code>torch.nn.Linear(in_features, out_features, bias=True)</code></p>
<p>对输入数据做线性变换：$y&#x3D;Ax+b$。</p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>in_features</strong> - 每个输入样本的大小</li>
<li><strong>out_features</strong> - 每个输出样本的大小</li>
<li><strong>bias</strong> - 若设置为False，这层不会学习偏置。默认值：True</li>
</ul>
<p><strong>形状：</strong></p>
<ul>
<li><strong>输入:</strong> $(N,in_features)$</li>
<li><strong>输出：</strong> $(N,out_features)$</li>
</ul>
<h5 id="Bilinear"><a href="#Bilinear" class="headerlink" title="Bilinear"></a>Bilinear</h5><p><code>torch.nn.Bilinear(in1_features, in2_features, out_features, bias=True)</code></p>
<p>对输入数据做双线性变换：$y&#x3D;x_1^TAx_2+b$。</p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>in1_features</strong> – size of each first input sample</li>
<li><strong>in2_features</strong> – size of each second input sample</li>
<li><strong>out_features</strong> – size of each output sample</li>
<li><strong>bias</strong> – If set to False, the layer will not learn an additive bias. Default: <code>True</code></li>
</ul>
<h4 id="Dropout-层"><a href="#Dropout-层" class="headerlink" title="Dropout 层"></a>Dropout 层</h4><h5 id="X-维-Dropout"><a href="#X-维-Dropout" class="headerlink" title="X 维 Dropout"></a>X 维 Dropout</h5><ul>
<li><code>torch.nn.Dropout(p=0.5, inplace=False)</code></li>
<li><code>torch.nn.Dropout2d(p=0.5, inplace=False)</code></li>
<li><code>torch.nn.Dropout3d(p=0.5, inplace=False)</code></li>
</ul>
<p>输入输出形状参照 卷积层—X 维卷积。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>每次优化循环的迭代为一个 <code>epoch</code>。</p>
<p>每个 <code>epoch</code> 由以下两个重要部分组成：</p>
<ul>
<li><strong>The Train Loop</strong> - 在训练数据集上迭代以收敛到最优参数。</li>
<li><strong>The Validation&#x2F;Test Loop</strong> - 在测试集上测试来检查模型性能是否有提升。</li>
</ul>
<h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>对于回归任务来说，常用的 loss function 包括均方误差 <code>nn.MSELoss</code>，对于分类任务来说，Negative Log Likehood <code>nn.NLLLoss</code>。还有将 <code>nn.LogSoftmax</code> 和 <code>nn.NLLLoss</code> 结合起来的 <code>nn.CrossEntropyLoss</code> 交叉熵 loss function。</p>
<h3 id="Autograd-求导"><a href="#Autograd-求导" class="headerlink" title="Autograd 求导"></a>Autograd 求导</h3><p><code>torch.autograd</code>提供了类和函数用来对任意标量函数进行求导。要想使用自动求导，只需要对已有的代码进行微小的改变。只需要将所有的<code>tensor</code>包含进<code>Variable</code>对象中即可。</p>
<p>可以通过创建 <code>tensor</code> 时设置 <code>requires_grad</code>，也可以通过 <code>x.requires_grad_(True)</code> 来设置自动求导。</p>
<p>可以通过 <code>z.grad_fn</code> 来查看 Gradient function。</p>
<p>在模型中，只需要 <code>loss.backward()</code> 就可以计算后向传播时的导数。</p>
<p>如果想要关闭梯度跟踪，则需要在 <code>with torch.no_grad():</code> 下进行函数操作，或者对 <code>tensor</code> 使用 <code>detach()</code>。</p>
<h3 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h3><p><code>torch.optim</code>是一个实现了各种优化算法的库。大部分常用的方法得到支持，并且接口具备足够的通用性，使得未来能够集成更加复杂的方法。</p>
<p>为了构建一个<code>Optimizer</code>，你需要给它一个包含了需要优化的参数（必须都是<code>Variable</code>对象）的iterable。然后，你可以设置optimizer的参数选项，比如学习率，权重衰减。</p>
<p>常用的 Optimizer 有 SGD 和 Adam。</p>
<p>所有 Optimizer 中都含有 <code>step()</code> 和 <code>zero_grad()</code> 函数， <code>step()</code> 函数完成单次优化迭代，<code>zero_grad()</code> 将之前计算的梯度清空。通用训练流程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="hljs-number">0.1</span>, momentum=<span class="hljs-number">0.9</span>)<br>optimizer.zero_grad()<br>loss(model(<span class="hljs-built_in">input</span>), target).backward()<br>optimizer.step()<br></code></pre></td></tr></table></figure>

<p>如果要对 learning_rate 采用特殊下降方法，则通用训练流程为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">scheduler = (optimizer, ... )<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    train(...)<br>    validate(...)<br>    scheduler.step()<br></code></pre></td></tr></table></figure>



<h4 id="迭代优化方法"><a href="#迭代优化方法" class="headerlink" title="迭代优化方法"></a>迭代优化方法</h4><h5 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h5><p><code>torch.optim.SGD(params, lr=&lt;required parameter&gt;, momentum=0, dampening=0, weight_decay=0, nesterov=False)</code></p>
<ul>
<li>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict</li>
<li>lr (<code>float</code>) – 学习率</li>
<li>momentum (<code>float</code>, 可选) – 动量因子（默认：0）</li>
<li>weight_decay (<code>float</code>, 可选) – 权重衰减（L2惩罚）（默认：0）</li>
<li>dampening (<code>float</code>, 可选) – 动量的抑制因子（默认：0）</li>
<li>nesterov (<code>bool</code>, 可选) – 使用Nesterov动量（默认：False）</li>
</ul>
<h5 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h5><p><code>torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)</code></p>
<ul>
<li>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict</li>
<li>lr (<code>float</code>, 可选) – 学习率（默认：1e-3）</li>
<li>betas (Tuple[<code>float</code>, <code>float</code>], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999）</li>
<li>eps (<code>float</code>, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）</li>
<li>weight_decay (<code>float</code>, 可选) – 权重衰减（L2惩罚）（默认: 0）</li>
</ul>
<h4 id="lr-scheduler"><a href="#lr-scheduler" class="headerlink" title="lr_scheduler"></a>lr_scheduler</h4><h5 id="余弦退火"><a href="#余弦退火" class="headerlink" title="余弦退火"></a>余弦退火</h5><p><code>torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False)</code></p>
<p><strong>参数：</strong></p>
<ul>
<li><strong>optimizer</strong> (<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer"><em>Optimizer</em></a>) – Wrapped optimizer.</li>
<li><strong>T_max</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – Maximum number of iterations.</li>
<li><strong>eta_min</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#float"><em>float</em></a>) – Minimum learning rate. Default: 0.</li>
<li><strong>last_epoch</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – The index of last epoch. Default: -1.</li>
<li><strong>verbose</strong> (<a target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a>) – If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</li>
</ul>
<h3 id="模型的存储和导入"><a href="#模型的存储和导入" class="headerlink" title="模型的存储和导入"></a>模型的存储和导入</h3><h4 id="存储模型权重"><a href="#存储模型权重" class="headerlink" title="存储模型权重"></a>存储模型权重</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision.models <span class="hljs-keyword">as</span> models<br><span class="hljs-comment"># 通过 state_dict() 方法存储权重</span><br>model = models.vgg16(pretrained=<span class="hljs-literal">True</span>)<br>torch.save(model.state_dict(), <span class="hljs-string">&#x27;model_weights.pth&#x27;</span>)<br><br><span class="hljs-comment"># 通过 load_state_dict() 方法导入权重</span><br>model = models.vgg16()<br>model.load_state_dict(torch.load(<span class="hljs-string">&#x27;model_weights.pth&#x27;</span>))<br></code></pre></td></tr></table></figure>

<h4 id="存储整个模型"><a href="#存储整个模型" class="headerlink" title="存储整个模型"></a>存储整个模型</h4><p>将整个模型结构和权重一起存储，数据较大。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 存储</span><br>torch.save(model, <span class="hljs-string">&#x27;model.pth&#x27;</span>)<br>model = torch.load(<span class="hljs-string">&#x27;model.pth&#x27;</span>)<br></code></pre></td></tr></table></figure>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Coding/" class="category-chain-item">Coding</a>
  
  
    <span>></span>
    
  <a href="/categories/Coding/PyTorch/" class="category-chain-item">PyTorch</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/PyTorch/">#PyTorch</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Pytorch Tutorial</div>
      <div>https://pandintelli.github.io/2022/01/13/Pytorch-Tutorial/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Pand</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年1月13日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/01/13/Blog-Remote-Deploy/" title="基于腾讯云的博客服务器部署">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">基于腾讯云的博客服务器部署</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/01/11/SimCLR/" title="论文阅读：A Simple Framework for Contrastive Learning of Visual Representations">
                        <span class="hidden-mobile">论文阅读：A Simple Framework for Contrastive Learning of Visual Representations</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
